{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notatki\n",
    "Tutorial tutaj: https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8\n",
    "\n",
    "Dane Flickr 8k:\n",
    "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "\n",
    "Polskie podpisy: http://zil.ipipan.waw.pl/Scwad/AIDe?action=AttachFile&do=get&target=AIDe_ANNOTATED_DESCRIPTIONS.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\macie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from statistics import mean\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_mode = 'lemma_no_pl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "doc = load_doc(f'../../AIDe_ANNOTATED_DESCRIPTIONS/token_{learn_mode}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_no_pl loaded: 1000 \n"
     ]
    }
   ],
   "source": [
    "# create dictionary 'mapping' that contains all names of images without \n",
    "# .jpg extention and their corresponding descriptons\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\t# process lines\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\t# take the first token as the image id, the rest as the description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# extract filename from image id\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\t# convert description tokens back to string\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\t# create the list if needed\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = list()\n",
    "\t\t# store description\n",
    "\t\tmapping[image_id].append(image_desc)\n",
    "\treturn mapping\n",
    "\n",
    "descriptions = load_descriptions(doc)\n",
    "print(learn_mode, 'loaded: %d ' % len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare translation table by removing numbers, non-letter characters and by lowercasing the text\n",
    "def clean_descriptions(descriptions):\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\t# tokenize\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\t# convert to lower case\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
    "\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_no_pl vocabulary Size: 3237\n"
     ]
    }
   ],
   "source": [
    "# Function for creating vocabulary of all words present from loaded descriptions\n",
    "def to_vocabulary(descriptions):\n",
    "\t# build a list of all description strings\n",
    "\tall_desc = set()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print(learn_mode, 'vocabulary Size: %d' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    train.append(load_set(f'randomState/trainImages{i}.txt'))\n",
    "    test.append(load_set(f'randomState/testImages{i}.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below path contains all the images\n",
    "images = '../../Flickr8k_Dataset/'\n",
    "# Create a list of all image names in the directory\n",
    "img = glob.glob(images + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_img = []\n",
    "\n",
    "# Read the train image names in a set\n",
    "for i in range(0,5):\n",
    "    train_images.append(set(open(f'randomState/trainImages{i}.txt', 'r').read().strip().split('\\n')))\n",
    "    # Create a list of all the training images with their full path names\n",
    "    train_img.append([])\n",
    "\n",
    "    for im in img: # img is list of full path names of all images\n",
    "        if im[len(images):] in train_images[i]: # Check if the image belongs to training set\n",
    "            train_img[i].append(im) # Add it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_img = []\n",
    "\n",
    "# Read the train image names in a set\n",
    "for i in range(0,5):\n",
    "    test_images.append(set(open(f'randomState/testImages{i}.txt', 'r').read().strip().split('\\n')))\n",
    "\n",
    "    # Create a list of all the training images with their full path names\n",
    "    test_img.append([])\n",
    "\n",
    "    for im in img: # img is list of full path names of all images\n",
    "        if im[len(images):] in test_images[i]: # Check if the image belongs to training set\n",
    "            test_img[i].append(im) # Add it to the list of train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: train 0 = 700\n",
      "Descriptions: test 0 = 300\n",
      "Descriptions: train no tokens 0 = 700\n",
      "Descriptions: train 1 = 700\n",
      "Descriptions: test 1 = 300\n",
      "Descriptions: train no tokens 1 = 700\n",
      "Descriptions: train 2 = 700\n",
      "Descriptions: test 2 = 300\n",
      "Descriptions: train no tokens 2 = 700\n",
      "Descriptions: train 3 = 700\n",
      "Descriptions: test 3 = 300\n",
      "Descriptions: train no tokens 3 = 700\n",
      "Descriptions: train 4 = 700\n",
      "Descriptions: test 4 = 300\n",
      "Descriptions: train no tokens 4 = 700\n"
     ]
    }
   ],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset, isForTraining):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tif isForTraining:\n",
    "\t\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\telse:\n",
    "\t\t\t\tdesc = ' '.join(image_desc)\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "train_descriptions = []\n",
    "test_descriptions = []\n",
    "no_tokens_train_descriptions = []\n",
    "\n",
    "# descriptions\n",
    "for i in range(0,5):\n",
    "    train_descriptions.append(load_clean_descriptions(f'../descriptions_{learn_mode}.txt', train[i], True))\n",
    "    print(f'Descriptions: train {i} = %d' % len(train_descriptions[i]))\n",
    "    test_descriptions.append(load_clean_descriptions(f'../descriptions_{learn_mode}.txt', test[i], False))\n",
    "    print(f'Descriptions: test {i} = %d' % len(test_descriptions[i]))\n",
    "    no_tokens_train_descriptions.append(load_clean_descriptions(f'../descriptions_{learn_mode}.txt', train[i], False))\n",
    "    print(f'Descriptions: train no tokens {i} = %d' % len(no_tokens_train_descriptions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    # Convert PIL image to numpy array of 3-dimensions\n",
    "    x = image.img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess the images using preprocess_input() from inception module\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inception v3 model\n",
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode a given image into a vector of size (2048, )\n",
    "def encode(image):\n",
    "    image = preprocess(image) # preprocess the image\n",
    "    fea_vec = model_new.predict(image) # Get the encoding vector for the image\n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
    "    return fea_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photos: train 0 = 700\n",
      "Photos: train 1 = 700\n",
      "Photos: train 2 = 700\n",
      "Photos: train 3 = 700\n",
      "Photos: train 4 = 700\n"
     ]
    }
   ],
   "source": [
    "train_features = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    train_features.append(load(open(f'Pickle/encoded_train_images_{learn_mode}_{i}.pkl', 'rb')))\n",
    "    print(f'Photos: train {i} = %d' % len(train_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "1400\n",
      "1400\n",
      "1400\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions\n",
    "all_train_captions = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    all_train_captions.append([])\n",
    "    for key, val in train_descriptions[i].items():\n",
    "        for cap in val:\n",
    "            all_train_captions[i].append(cap)\n",
    "    print(len(all_train_captions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed words 2115 -> 236\n",
      "preprocessed words 2100 -> 244\n",
      "preprocessed words 2077 -> 245\n",
      "preprocessed words 2075 -> 245\n",
      "preprocessed words 2054 -> 244\n"
     ]
    }
   ],
   "source": [
    "# Consider only words which occur at least 10 times in the corpus\n",
    "word_count_threshold = 10\n",
    "word_counts = []\n",
    "vocab = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    word_counts.append({})\n",
    "    nsents = 0\n",
    "    for sent in all_train_captions[i]:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[i][w] = word_counts[i].get(w, 0) + 1\n",
    "\n",
    "    vocab.append([w for w in word_counts[i] if word_counts[i][w] >= word_count_threshold])\n",
    "    print('preprocessed words %d -> %d' % (len(word_counts[i]), len(vocab[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtoword = []\n",
    "wordtoix = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    ixtoword.append({})\n",
    "    wordtoix.append({})\n",
    "\n",
    "    ix = 1\n",
    "    for w in vocab[i]:\n",
    "        wordtoix[i][w] = ix\n",
    "        ixtoword[i][ix] = w\n",
    "        ix += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[237, 245, 246, 246, 245]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    vocab_size.append(len(ixtoword[i]) + 1) # one for appended 0's\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 31\n",
      "Description Length: 31\n",
      "Description Length: 36\n",
      "Description Length: 36\n",
      "Description Length: 36\n"
     ]
    }
   ],
   "source": [
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def get_max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    "\n",
    "max_length = []\n",
    "\n",
    "# determine the maximum sequence length\n",
    "for i in range(0,5):\n",
    "    max_length.append(get_max_length(train_descriptions[i]))\n",
    "    print('Description Length: %d' % max_length[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedings pobrane stÄ…d: https://github.com/sdadas/polish-nlp-resources/releases/download/v1.0/glove.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Glove vectors\n",
    "glove_dir = '../../glove'\n",
    "embeddings_index = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    embeddings_index.append({}) # empty dictionary\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove_100_3_polish.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    for i in range(0,5):\n",
    "        word = values[0] if learn_mode != 'no_pl' else unidecode(values[0])\n",
    "        embeddings_index[i][word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = []\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "for i in range(0,5):\n",
    "    embedding_matrix.append(np.zeros((vocab_size[i], embedding_dim)))\n",
    "\n",
    "    for word, j in wordtoix[i].items():\n",
    "        #if i < max_words:\n",
    "        embedding_vector = embeddings_index[i].get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in the embedding index will be all zeros\n",
    "            embedding_matrix[i][j] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    inputs2 = Input(shape=(max_length[i],))\n",
    "    se1 = Embedding(vocab_size[i], embedding_dim, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size[i], activation='softmax')(decoder2)\n",
    "    model.append(Model(inputs=[inputs1, inputs2], outputs=outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    model[i].layers[2].set_weights([embedding_matrix[i]])\n",
    "    model[i].layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    model[i].compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    model[i].load_weights(f'model_weights/model_30_{learn_mode}_{i}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = '../../Flickr8k_Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train = []\n",
    "encoding_test = []\n",
    "\n",
    "#load image features from pickle files\n",
    "for i in range(0,5):\n",
    "    with open(f'Pickle/encoded_train_images_{learn_mode}_{i}.pkl', 'rb') as encoded_pickle:\n",
    "        encoding_train.append(load(encoded_pickle))\n",
    "    with open(f'Pickle/encoded_test_images_{learn_mode}_{i}.pkl', 'rb') as encoded_pickle:\n",
    "        encoding_test.append(load(encoded_pickle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for generating captions for image. It is greedy function because it predicts next best possible word \n",
    "#in sentence given image features and previous predicted word in the sentence. Alghorithm stops when it reaches\n",
    "#max possible length of caption or the word endseq.\n",
    "\n",
    "def greedy_search(photo, index):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length[index]):\n",
    "        sequence = [wordtoix[index][w] for w in in_text.split() if w in wordtoix[index]]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length[index])\n",
    "        yhat = model[index].predict([photo,sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[index][yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sources:\n",
    "#https://www.researchgate.net/publication/268689555_CIDEr_Consensus-based_Image_Description_Evaluation\n",
    "#https://www.sciencedirect.com/topics/computer-science/cosine-similarity\n",
    "#https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/\n",
    "\n",
    "#function for calculating cider metric\n",
    "def cider(given_captions, prediction):\n",
    "    prediction_arr = []\n",
    "    prediction_arr.append(prediction)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    trainedV = vectorizer.fit(given_captions)\n",
    "    givenTfIdf = trainedV.transform(given_captions).toarray()\n",
    "    predTfIdf = trainedV.transform(prediction_arr).toarray()\n",
    "    partialCiders = 0\n",
    "    help_arr = []\n",
    "    \n",
    "    for c in givenTfIdf:\n",
    "        help_arr.append(c)\n",
    "        partialCiders = partialCiders + cosine_similarity(predTfIdf,help_arr)\n",
    "        help_arr = []\n",
    "        \n",
    "    return partialCiders/len(givenTfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words = bow\n",
    "\n",
    "def get_bow(description):\n",
    "    bow = description.split(' ')\n",
    "    bow = set(bow)\n",
    "    return bow\n",
    "\n",
    "#calculate bow score\n",
    "def bow_score(bow_train, bow_pred):\n",
    "    overlap = bow_train & bow_pred\n",
    "    universe = bow_train | bow_pred\n",
    "\n",
    "    result1 = float(len(overlap)) / len(bow_train) * 100\n",
    "    result2 = float(len(overlap)) / len(bow_pred) * 100\n",
    "    result3 = float(len(overlap)) / len(universe) * 100\n",
    "    return result1, len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 4 299\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bleu TREN</th>\n",
       "      <th>Bleu TEST</th>\n",
       "      <th>Meteor TREN</th>\n",
       "      <th>Meteor TEST</th>\n",
       "      <th>CIDEr TREN</th>\n",
       "      <th>CIDEr TEST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043925</td>\n",
       "      <td>0.008942</td>\n",
       "      <td>0.190126</td>\n",
       "      <td>0.128490</td>\n",
       "      <td>0.281820</td>\n",
       "      <td>0.227051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024209</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>0.172188</td>\n",
       "      <td>0.110724</td>\n",
       "      <td>0.279981</td>\n",
       "      <td>0.226442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033593</td>\n",
       "      <td>0.005390</td>\n",
       "      <td>0.203894</td>\n",
       "      <td>0.149129</td>\n",
       "      <td>0.304074</td>\n",
       "      <td>0.253831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023446</td>\n",
       "      <td>0.009099</td>\n",
       "      <td>0.165780</td>\n",
       "      <td>0.122720</td>\n",
       "      <td>0.256607</td>\n",
       "      <td>0.222140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040174</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.204667</td>\n",
       "      <td>0.130822</td>\n",
       "      <td>0.304884</td>\n",
       "      <td>0.243957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bleu TREN  Bleu TEST  Meteor TREN  Meteor TEST  CIDEr TREN  CIDEr TEST\n",
       "0   0.043925   0.008942     0.190126     0.128490    0.281820    0.227051\n",
       "1   0.024209   0.005275     0.172188     0.110724    0.279981    0.226442\n",
       "2   0.033593   0.005390     0.203894     0.149129    0.304074    0.253831\n",
       "3   0.023446   0.009099     0.165780     0.122720    0.256607    0.222140\n",
       "4   0.040174   0.006343     0.204667     0.130822    0.304884    0.243957"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#preparation of metrics lists\n",
    "bleu_train_scores = []\n",
    "meteor_train_scores = []\n",
    "cider_train_scores = []\n",
    "\n",
    "bleu_test_scores = []    \n",
    "meteor_test_scores = []\n",
    "cider_test_scores = []\n",
    "    \n",
    "for i in range(0,5):\n",
    "    bleu_train_scores.append([])\n",
    "    meteor_train_scores.append([])\n",
    "    cider_train_scores.append([])\n",
    "\n",
    "    bleu_test_scores.append([])\n",
    "    meteor_test_scores.append([])\n",
    "    cider_test_scores.append([])\n",
    "    \n",
    "    train_pics = list(encoding_train[i].keys())\n",
    "    test_pics = list(encoding_test[i].keys())\n",
    "\n",
    "    #calculation of metrics for train images dataset\n",
    "    for j in range(0,len(train_pics)):\n",
    "        pic = train_pics[j]\n",
    "        image = encoding_train[i][pic].reshape((1,2048))\n",
    "\n",
    "        actual_desc_0 = no_tokens_train_descriptions[i][pic[:-4]][0]\n",
    "        actual_desc_1 = no_tokens_train_descriptions[i][pic[:-4]][1]\n",
    "        generated = greedy_search(image,i)\n",
    "        bleu_train_scores[i].append(\n",
    "            sentence_bleu(\n",
    "                [actual_desc_0.split(), actual_desc_1.split()],\n",
    "                generated.split())\n",
    "        )\n",
    "        meteor_train_scores[i].append(meteor_score(no_tokens_train_descriptions[i][pic.split('.')[0]],generated))\n",
    "        cider_train_scores[i].append(cider(no_tokens_train_descriptions[i][pic.split('.')[0]],generated)[0][0])\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print('train',i,j, flush=True)\n",
    "\n",
    "    #calculation of metrics for test images dataset\n",
    "    for j in range(0,len(test_pics)):\n",
    "        pic = test_pics[j]\n",
    "        image = encoding_test[i][pic].reshape((1,2048))\n",
    "\n",
    "        actual_desc_0 = test_descriptions[i][pic[:-4]][0]\n",
    "        actual_desc_1 = test_descriptions[i][pic[:-4]][1]\n",
    "        generated = greedy_search(image,i)\n",
    "        bleu_test_scores[i].append(\n",
    "            sentence_bleu(\n",
    "                [actual_desc_0.split(), actual_desc_1.split()],\n",
    "                generated.split())\n",
    "        )\n",
    "        meteor_test_scores[i].append(meteor_score(test_descriptions[i][pic.split('.')[0]],generated))\n",
    "        cider_test_scores[i].append(cider(test_descriptions[i][pic.split('.')[0]],generated)[0][0])\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print('test',i,j, flush=True)\n",
    "\n",
    "\n",
    "result_data = {}\n",
    "\n",
    "#calculate mean score based on socres list for each metric\n",
    "for i in range(0,5):\n",
    "    result_data[str(i)] = [\n",
    "        mean(bleu_train_scores[i]),\n",
    "        mean(bleu_test_scores[i]),\n",
    "        mean(meteor_train_scores[i]),\n",
    "        mean(meteor_test_scores[i]),\n",
    "        mean(cider_train_scores[i]),\n",
    "        mean(cider_test_scores[i])\n",
    "    ]\n",
    "\n",
    "result = pd.DataFrame.from_dict(\n",
    "    result_data,\n",
    "    orient='index',\n",
    "    columns=['Bleu TREN', 'Bleu TEST', 'Meteor TREN', 'Meteor TEST', 'CIDEr TREN', 'CIDEr TEST']\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
